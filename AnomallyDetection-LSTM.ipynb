{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Lab4(2).ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python383jvsc74a57bd0e95bd23c58372e532c13f19770ac32345ce14fa00d207afebbf1161aa1e79f4b",
      "display_name": "Python 3.8.3 64-bit (conda)"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vY4SK0xKAJgm"
      },
      "source": [
        "# DATA 586 - Project - LSTM Neuro Net"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "moNmVfuvnImW"
      },
      "source": [
        "# import all required packages\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "from torchtext import data\n",
        "from torchtext import datasets\n",
        "import time\n",
        "import random\n",
        "import pandas as pd\n",
        "import torchtext.legacy #used for NLP\n",
        "\n",
        "torch.backends.cudnn.deterministic = True"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OvW1RgfepCBq"
      },
      "source": [
        "# Set parameters for the LSTM\n",
        "\n",
        "RANDOM_SEED = 123\n",
        "torch.manual_seed(RANDOM_SEED)\n",
        "\n",
        "VOCABULARY_SIZE = 22000\n",
        "LEARNING_RATE = 1e-3\n",
        "BATCH_SIZE = 128\n",
        "NUM_EPOCHS = 25\n",
        "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "EMBEDDING_DIM = 220\n",
        "HIDDEN_DIM = 440\n",
        "OUTPUT_DIM = 1"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mQMmKUEisW4W"
      },
      "source": [
        "## Dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 413
        },
        "id": "-ndaR73DDrqD",
        "outputId": "f78486b6-0c19-4615-fb21-4da58b818cd9"
      },
      "source": [
        "# Review the data set before running the neuro net\n",
        "# This cell is not used when building or running the neuro net\n",
        "df = pd.read_csv('HDFS_2k-parsed_3-compl.csv', delimiter = \"\\t\")\n",
        "df.head()"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                                    TagCode,LogLabel\n",
              "0  1 0   1 0   1 0   1   8   9   6   9   0   0   ...\n",
              "1  1   1 0   1 0   1 0   0   0   0   7   9   8   ...\n",
              "2  1 0   1 0   1   1 0   6   9   7   9   8   9   ...\n",
              "3  1 0   1   1 0   1 0   0   0   0   8   9   7   ...\n",
              "4  1 0   1 0   1 0   1   6   9   8   9   7   9   ..."
            ],
            "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>TagCode,LogLabel</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>1 0   1 0   1 0   1   8   9   6   9   0   0   ...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>1   1 0   1 0   1 0   0   0   0   7   9   8   ...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>1 0   1 0   1   1 0   6   9   7   9   8   9   ...</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>1 0   1   1 0   1 0   0   0   0   8   9   7   ...</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>1 0   1 0   1 0   1   6   9   8   9   7   9   ...</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d0BvBUXjDrqD"
      },
      "source": [
        "# Delete info from previous cell\n",
        "del df"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zzxkTPNvDrqF"
      },
      "source": [
        "# Define the label and text parameters\n",
        "\n",
        "TEXT = torchtext.legacy.data.Field(sequential=True,\n",
        "                  tokenize='spacy',\n",
        "                  include_lengths=True) # necessary for packed_padded_sequence\n",
        "\n",
        "LABEL = torchtext.legacy.data.LabelField(dtype=torch.float)"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "C:\\Users\\Graha\\miniconda3\\lib\\site-packages\\torchtext\\data\\utils.py:123: UserWarning: Spacy model \"en\" could not be loaded, trying \"en_core_web_sm\" instead\n  warnings.warn(f'Spacy model \"{language}\" could not be loaded, trying \"{OLD_MODEL_SHORTCUTS[language]}\" instead')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OrBRc-LXDrqF"
      },
      "source": [
        "# Assign fields ('headers of csv')\n",
        "# Get data from csv\n",
        "fields = [('review', TEXT), ('sentiment', LABEL)]\n",
        "\n",
        "dataset = torchtext.legacy.data.TabularDataset(\n",
        "    path=\"HDFS_2k-parsed_3-compl.csv\", format='csv',\n",
        "    skip_header=True, fields=fields)"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WZ_4jiHVnMxN"
      },
      "source": [
        "# Split data into training, validation and tests sets\n",
        "\n",
        "train_data, valid_data, test_data = dataset.split(\n",
        "    split_ratio=[0.75, 0.05, 0.2],\n",
        "    random_state=random.seed(RANDOM_SEED))\n",
        "\n",
        "print(f'Num Train: {len(train_data)}')\n",
        "print(f'Num Valid: {len(valid_data)}')\n",
        "print(f'Num Test: {len(test_data)}')"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Num Train: 431296\nNum Valid: 115012\nNum Test: 28753\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e8uNrjdtn4A8"
      },
      "source": [
        "# Build vocabulary\n",
        "TEXT.build_vocab(train_data, max_size=VOCABULARY_SIZE)\n",
        "LABEL.build_vocab(train_data)\n",
        "\n",
        "print(f'Vocabulary size: {len(TEXT.vocab)}')\n",
        "print(f'Number of classes: {len(LABEL.vocab)}')"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Vocabulary size: 13\nNumber of classes: 2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OuVBXHVbDrqH"
      },
      "source": [
        "# Display count for normal and anomaly logs\n",
        "LABEL.vocab.freqs"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Counter({'1': 418705, '0': 12591})"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eIQ_zfKLwjKm"
      },
      "source": [
        "Make dataset iterators:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i7JiHR1stHNF"
      },
      "source": [
        "train_loader, valid_loader, test_loader = torchtext.legacy.data.BucketIterator.splits(\n",
        "    (train_data, valid_data, test_data), \n",
        "    batch_size=BATCH_SIZE,\n",
        "    sort_within_batch=True, # necessary for packed_padded_sequence\n",
        "    sort_key=lambda x: len(x.review),\n",
        "    device=DEVICE)"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y8SP_FccutT0"
      },
      "source": [
        "# Test the iterators\n",
        "print('Train')\n",
        "for batch in train_loader:\n",
        "    print(f'Text matrix size: {batch.review[0].size()}')\n",
        "    print(f'Target vector size: {batch.sentiment.size()}')\n",
        "    break\n",
        "    \n",
        "print('\\nValid:')\n",
        "for batch in valid_loader:\n",
        "    print(f'Text matrix size: {batch.review[0].size()}')\n",
        "    print(f'Target vector size: {batch.sentiment.size()}')\n",
        "    break\n",
        "    \n",
        "print('\\nTest:')\n",
        "for batch in test_loader:\n",
        "    print(f'Text matrix size: {batch.review[0].size()}')\n",
        "    print(f'Target vector size: {batch.sentiment.size()}')\n",
        "    break"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train\n",
            "Text matrix size: torch.Size([40, 128])\n",
            "Target vector size: torch.Size([128])\n",
            "\n",
            "Valid:\n",
            "Text matrix size: torch.Size([4, 128])\n",
            "Target vector size: torch.Size([128])\n",
            "\n",
            "Test:\n",
            "Text matrix size: torch.Size([4, 128])\n",
            "Target vector size: torch.Size([128])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Neuro net model\n",
        "import torch.nn as nn\n",
        "\n",
        "class RNN(nn.Module):\n",
        "    def __init__(self, input_dim, embedding_dim, hidden_dim, output_dim):\n",
        "        \n",
        "        super().__init__()\n",
        "\n",
        "        self.embedding = nn.Embedding(input_dim, embedding_dim)\n",
        "        self.rnn = nn.LSTM(embedding_dim, hidden_dim)\n",
        "        self.fc = nn.Linear(hidden_dim, output_dim)\n",
        "        \n",
        "    def forward(self, text, text_length):\n",
        "\n",
        "        #[sentence len, batch size] => [sentence len, batch size, embedding size]\n",
        "        embedded = self.embedding(text)\n",
        "        \n",
        "        packed = torch.nn.utils.rnn.pack_padded_sequence(embedded, text_length)\n",
        "        \n",
        "        #[sentence len, batch size, embedding size] => \n",
        "        #  output: [sentence len, batch size, hidden size]\n",
        "        #  hidden: [1, batch size, hidden size]\n",
        "        packed_output, (hidden, cell) = self.rnn(packed)\n",
        "        \n",
        "        return self.fc(hidden.squeeze(0)).view(-1) # return 0 or 1"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ik3NF3faxFmZ"
      },
      "source": [
        "INPUT_DIM = len(TEXT.vocab)\n",
        "\n",
        "torch.manual_seed(RANDOM_SEED)\n",
        "model = RNN(INPUT_DIM, EMBEDDING_DIM, HIDDEN_DIM, OUTPUT_DIM)\n",
        "model = model.to(DEVICE)\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=LEARNING_RATE)"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Lv9Ny9di6VcI"
      },
      "source": [
        "## Training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T5t1Afn4xO11"
      },
      "source": [
        "def compute_binary_accuracy(model, data_loader, device):\n",
        "    model.eval()\n",
        "    correct_pred, num_examples = 0, 0\n",
        "    with torch.no_grad():\n",
        "        for batch_idx, batch_data in enumerate(data_loader):\n",
        "            text, text_lengths = batch_data.review\n",
        "            logits = model(text, text_lengths.cpu())\n",
        "            predicted_labels = (torch.sigmoid(logits) > 0.5).long()\n",
        "            num_examples += batch_data.sentiment.size(0)\n",
        "            correct_pred += (predicted_labels.long() == batch_data.sentiment.long()).sum()\n",
        "        return correct_pred.float()/num_examples * 100"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EABZM8Vo0ilB"
      },
      "source": [
        "start_time = time.time()\n",
        "\n",
        "for epoch in range(NUM_EPOCHS):\n",
        "    model.train()\n",
        "    for batch_idx, batch_data in enumerate(train_loader):\n",
        "        \n",
        "        text, text_lengths = batch_data.review\n",
        "        \n",
        "        ### FORWARD AND BACK PROP\n",
        "        logits = model(text, text_lengths.cpu())\n",
        "        cost = F.binary_cross_entropy_with_logits(logits, batch_data.sentiment)\n",
        "        optimizer.zero_grad()\n",
        "        \n",
        "        cost.backward()\n",
        "        \n",
        "        ### UPDATE MODEL PARAMETERS\n",
        "        optimizer.step()\n",
        "        \n",
        "        ### LOGGING\n",
        "        if not batch_idx % 50:\n",
        "            print (f'Epoch: {epoch+1:03d}/{NUM_EPOCHS:03d} | '\n",
        "                   f'Batch {batch_idx:03d}/{len(train_loader):03d} | '\n",
        "                   f'Cost: {cost:.4f}')\n",
        "\n",
        "    with torch.set_grad_enabled(False):\n",
        "        print(f'training accuracy: '\n",
        "              f'{compute_binary_accuracy(model, train_loader, DEVICE):.2f}%'\n",
        "              f'\\nvalid accuracy: '\n",
        "              f'{compute_binary_accuracy(model, valid_loader, DEVICE):.2f}%')\n",
        "        \n",
        "    print(f'Time elapsed: {(time.time() - start_time)/60:.2f} min')\n",
        "    \n",
        "print(f'Total Training Time: {(time.time() - start_time)/60:.2f} min')\n",
        "print(f'Test accuracy: {compute_binary_accuracy(model, test_loader, DEVICE):.2f}%')"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 001/025 | Batch 000/012 | Cost: 0.7108\n",
            "training accuracy: 96.46%\n",
            "valid accuracy: 96.23%\n",
            "Time elapsed: 0.01 min\n",
            "Epoch: 002/025 | Batch 000/012 | Cost: 0.1584\n",
            "training accuracy: 96.46%\n",
            "valid accuracy: 96.23%\n",
            "Time elapsed: 0.02 min\n",
            "Epoch: 003/025 | Batch 000/012 | Cost: 0.0908\n",
            "training accuracy: 96.46%\n",
            "valid accuracy: 96.23%\n",
            "Time elapsed: 0.02 min\n",
            "Epoch: 004/025 | Batch 000/012 | Cost: 0.1080\n",
            "training accuracy: 96.46%\n",
            "valid accuracy: 96.23%\n",
            "Time elapsed: 0.03 min\n",
            "Epoch: 005/025 | Batch 000/012 | Cost: 0.0598\n",
            "training accuracy: 96.46%\n",
            "valid accuracy: 96.23%\n",
            "Time elapsed: 0.04 min\n",
            "Epoch: 006/025 | Batch 000/012 | Cost: 0.0582\n",
            "training accuracy: 96.52%\n",
            "valid accuracy: 96.23%\n",
            "Time elapsed: 0.04 min\n",
            "Epoch: 007/025 | Batch 000/012 | Cost: 0.1627\n",
            "training accuracy: 96.52%\n",
            "valid accuracy: 96.23%\n",
            "Time elapsed: 0.05 min\n",
            "Epoch: 008/025 | Batch 000/012 | Cost: 0.1616\n",
            "training accuracy: 96.52%\n",
            "valid accuracy: 96.23%\n",
            "Time elapsed: 0.06 min\n",
            "Epoch: 009/025 | Batch 000/012 | Cost: 0.0832\n",
            "training accuracy: 96.52%\n",
            "valid accuracy: 96.23%\n",
            "Time elapsed: 0.07 min\n",
            "Epoch: 010/025 | Batch 000/012 | Cost: 0.0789\n",
            "training accuracy: 96.52%\n",
            "valid accuracy: 96.23%\n",
            "Time elapsed: 0.08 min\n",
            "Epoch: 011/025 | Batch 000/012 | Cost: 0.1922\n",
            "training accuracy: 96.52%\n",
            "valid accuracy: 96.23%\n",
            "Time elapsed: 0.08 min\n",
            "Epoch: 012/025 | Batch 000/012 | Cost: 0.1935\n",
            "training accuracy: 96.52%\n",
            "valid accuracy: 96.23%\n",
            "Time elapsed: 0.09 min\n",
            "Epoch: 013/025 | Batch 000/012 | Cost: 0.2237\n",
            "training accuracy: 96.52%\n",
            "valid accuracy: 96.23%\n",
            "Time elapsed: 0.10 min\n",
            "Epoch: 014/025 | Batch 000/012 | Cost: 0.0885\n",
            "training accuracy: 96.52%\n",
            "valid accuracy: 96.23%\n",
            "Time elapsed: 0.10 min\n",
            "Epoch: 015/025 | Batch 000/012 | Cost: 0.0845\n",
            "training accuracy: 96.52%\n",
            "valid accuracy: 96.23%\n",
            "Time elapsed: 0.11 min\n",
            "Epoch: 016/025 | Batch 000/012 | Cost: 0.1678\n",
            "training accuracy: 96.52%\n",
            "valid accuracy: 96.23%\n",
            "Time elapsed: 0.12 min\n",
            "Epoch: 017/025 | Batch 000/012 | Cost: 0.1375\n",
            "training accuracy: 96.52%\n",
            "valid accuracy: 96.23%\n",
            "Time elapsed: 0.13 min\n",
            "Epoch: 018/025 | Batch 000/012 | Cost: 0.1125\n",
            "training accuracy: 96.52%\n",
            "valid accuracy: 96.23%\n",
            "Time elapsed: 0.13 min\n",
            "Epoch: 019/025 | Batch 000/012 | Cost: 0.1888\n",
            "training accuracy: 96.52%\n",
            "valid accuracy: 96.23%\n",
            "Time elapsed: 0.14 min\n",
            "Epoch: 020/025 | Batch 000/012 | Cost: 0.1104\n",
            "training accuracy: 96.52%\n",
            "valid accuracy: 96.23%\n",
            "Time elapsed: 0.15 min\n",
            "Epoch: 021/025 | Batch 000/012 | Cost: 0.1110\n",
            "training accuracy: 96.52%\n",
            "valid accuracy: 96.23%\n",
            "Time elapsed: 0.15 min\n",
            "Epoch: 022/025 | Batch 000/012 | Cost: 0.0814\n",
            "training accuracy: 96.52%\n",
            "valid accuracy: 96.23%\n",
            "Time elapsed: 0.17 min\n",
            "Epoch: 023/025 | Batch 000/012 | Cost: 0.1173\n",
            "training accuracy: 96.52%\n",
            "valid accuracy: 96.23%\n",
            "Time elapsed: 0.18 min\n",
            "Epoch: 024/025 | Batch 000/012 | Cost: 0.0832\n",
            "training accuracy: 96.52%\n",
            "valid accuracy: 96.23%\n",
            "Time elapsed: 0.19 min\n",
            "Epoch: 025/025 | Batch 000/012 | Cost: 0.1911\n",
            "training accuracy: 96.52%\n",
            "valid accuracy: 96.23%\n",
            "Time elapsed: 0.20 min\n",
            "Total Training Time: 0.20 min\n",
            "Test accuracy: 100.00%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jt55pscgFdKZ"
      },
      "source": [
        "import spacy\n",
        "#nlp = spacy.load('en')\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "\n",
        "def predict_sentiment(model, sentence):\n",
        "    # based on:\n",
        "    # https://github.com/bentrevett/pytorch-sentiment-analysis/blob/\n",
        "    # master/2%20-%20Upgraded%20Sentiment%20Analysis.ipynb\n",
        "    model.eval()\n",
        "    tokenized = [tok.text for tok in nlp.tokenizer(sentence)]\n",
        "    indexed = [TEXT.vocab.stoi[t] for t in tokenized]\n",
        "    length = [len(indexed)]\n",
        "    tensor = torch.LongTensor(indexed).to(DEVICE)\n",
        "    tensor = tensor.unsqueeze(1)\n",
        "    length_tensor = torch.LongTensor(length)\n",
        "    prediction = torch.sigmoid(model(tensor, length_tensor))\n",
        "    return prediction.item()"
      ],
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O4__q0coFJyw"
      },
      "source": [
        "print('Probability positive:')\n",
        "1-predict_sentiment(model, \"This is such an awesome movie, I really love it!\")"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Probability positive:\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.5292898118495941"
            ]
          },
          "metadata": {},
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0BbCLOTADrqK"
      },
      "source": [
        "print('Probability negative:')\n",
        "predict_sentiment(model, \"I really hate this movie. It is really bad and sucks!\")"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Probability negative:\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.475669264793396"
            ]
          },
          "metadata": {},
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8VWjQpNgrkXc"
      },
      "source": [
        "**LSTM**\n",
        "\n",
        "By changing the following parameters we obtained a test accuracy of 91.24%.\n",
        "\n",
        "*   Vocabullary size: 20000 --> 22000\n",
        "*   Learning rate: 1e-4 --> 1e-3\n",
        "*   Embedding Dim: 128 --> 220\n",
        "*   Hidden Dim: 256 --> 440\n",
        "\n",
        "A small learning rate can cause a neuro net to get stuck in local minima. By increasing the learning we're allowing the neuro net to reach the global minima.\n",
        "\n",
        "Increasing the number of hidden and embedded dimensions increases the complexity of the model allowing the LSTM to account for more complex models. If the number of hidden dimensions is too high it will lead to overfitting.\n",
        "\n",
        "Increasing the vocab size allows the model to work with a larger dataset increasing the amount of data used doing the training and testing phase."
      ]
    },
    {
      "source": [
        "**Changes made to the GRU**\n",
        "\n",
        "After running the GRU, without changing any parameters from the LSTM the initial testing accuracy was 91.0% which is a similar accuracy to the LSTM. After changing the epoch, learning rate and number of dimensions there were no improvements to the accuracy of the model."
      ],
      "cell_type": "markdown",
      "metadata": {}
    }
  ]
}