{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vY4SK0xKAJgm"
      },
      "source": [
        "LSTM Neuro Net for Anomally Detection"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "moNmVfuvnImW"
      },
      "outputs": [],
      "source": [
        "# import all required packages\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "from torchtext import data\n",
        "from torchtext import datasets\n",
        "import time\n",
        "import random\n",
        "import pandas as pd\n",
        "import torchtext.legacy #used for NLP\n",
        "\n",
        "torch.backends.cudnn.deterministic = True"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "OvW1RgfepCBq"
      },
      "outputs": [],
      "source": [
        "# Set parameters for the LSTM\n",
        "\n",
        "RANDOM_SEED = 123\n",
        "torch.manual_seed(RANDOM_SEED)\n",
        "\n",
        "VOCABULARY_SIZE = 22000\n",
        "LEARNING_RATE = 1e-3\n",
        "BATCH_SIZE = 128\n",
        "NUM_EPOCHS = 25\n",
        "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "EMBEDDING_DIM = 220\n",
        "HIDDEN_DIM = 440\n",
        "OUTPUT_DIM = 1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mQMmKUEisW4W"
      },
      "source": [
        "## Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 413
        },
        "id": "-ndaR73DDrqD",
        "outputId": "f78486b6-0c19-4615-fb21-4da58b818cd9"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>review,sentiment</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>2,1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>4,1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>8,1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0,1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>6,1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "  review,sentiment\n",
              "0              2,1\n",
              "1              4,1\n",
              "2              8,1\n",
              "3              0,1\n",
              "4              6,1"
            ]
          },
          "execution_count": 3,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Review the data set before running the neuro net\n",
        "# This cell is not used when building or running the neuro net\n",
        "df = pd.read_csv('HDFS_2k-parsed_3-compl.csv', delimiter = \"\\t\")\n",
        "df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "d0BvBUXjDrqD"
      },
      "outputs": [],
      "source": [
        "# Delete info from previous cell\n",
        "del df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "zzxkTPNvDrqF"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "C:\\Users\\Graha\\miniconda3\\lib\\site-packages\\torchtext\\data\\utils.py:123: UserWarning: Spacy model \"en\" could not be loaded, trying \"en_core_web_sm\" instead\n",
            "  warnings.warn(f'Spacy model \"{language}\" could not be loaded, trying \"{OLD_MODEL_SHORTCUTS[language]}\" instead')\n"
          ]
        }
      ],
      "source": [
        "# Define the label and text parameters\n",
        "\n",
        "TEXT = torchtext.legacy.data.Field(sequential=True,\n",
        "                  tokenize='spacy',\n",
        "                  include_lengths=True) # necessary for packed_padded_sequence\n",
        "\n",
        "LABEL = torchtext.legacy.data.LabelField(dtype=torch.float)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "OrBRc-LXDrqF"
      },
      "outputs": [],
      "source": [
        "# Assign fields ('headers of csv')\n",
        "# Get data from csv\n",
        "fields = [('review', TEXT), ('sentiment', LABEL)]\n",
        "\n",
        "dataset = torchtext.legacy.data.TabularDataset(\n",
        "    path=\"HDFS_2k-parsed_3-compl.csv\", format='csv',\n",
        "    skip_header=True, fields=fields)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "WZ_4jiHVnMxN"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Num Train: 1495\n",
            "Num Valid: 398\n",
            "Num Test: 100\n"
          ]
        }
      ],
      "source": [
        "# Split data into training, validation and tests sets\n",
        "\n",
        "train_data, valid_data, test_data = dataset.split(\n",
        "    split_ratio=[0.75, 0.05, 0.2],\n",
        "    random_state=random.seed(RANDOM_SEED))\n",
        "\n",
        "print(f'Num Train: {len(train_data)}')\n",
        "print(f'Num Valid: {len(valid_data)}')\n",
        "print(f'Num Test: {len(test_data)}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "e8uNrjdtn4A8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Vocabulary size: 13\n",
            "Number of classes: 2\n"
          ]
        }
      ],
      "source": [
        "# Build vocabulary\n",
        "TEXT.build_vocab(train_data, max_size=VOCABULARY_SIZE)\n",
        "LABEL.build_vocab(train_data)\n",
        "\n",
        "print(f'Vocabulary size: {len(TEXT.vocab)}')\n",
        "print(f'Number of classes: {len(LABEL.vocab)}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "OuVBXHVbDrqH"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "Counter({'1': 1444, '0': 51})"
            ]
          },
          "execution_count": 9,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Display count for normal and anomaly logs\n",
        "LABEL.vocab.freqs"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eIQ_zfKLwjKm"
      },
      "source": [
        "Make dataset iterators:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "i7JiHR1stHNF"
      },
      "outputs": [],
      "source": [
        "train_loader, valid_loader, test_loader = torchtext.legacy.data.BucketIterator.splits(\n",
        "    (train_data, valid_data, test_data), \n",
        "    batch_size=BATCH_SIZE,\n",
        "    sort_within_batch=True, # necessary for packed_padded_sequence\n",
        "    sort_key=lambda x: len(x.review),\n",
        "    device=DEVICE)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "y8SP_FccutT0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Train\n",
            "Text matrix size: torch.Size([1, 128])\n",
            "Target vector size: torch.Size([128])\n",
            "\n",
            "Valid:\n",
            "Text matrix size: torch.Size([1, 128])\n",
            "Target vector size: torch.Size([128])\n",
            "\n",
            "Test:\n",
            "Text matrix size: torch.Size([2, 100])\n",
            "Target vector size: torch.Size([100])\n"
          ]
        }
      ],
      "source": [
        "# Test the iterators\n",
        "print('Train')\n",
        "for batch in train_loader:\n",
        "    print(f'Text matrix size: {batch.review[0].size()}')\n",
        "    print(f'Target vector size: {batch.sentiment.size()}')\n",
        "    break\n",
        "    \n",
        "print('\\nValid:')\n",
        "for batch in valid_loader:\n",
        "    print(f'Text matrix size: {batch.review[0].size()}')\n",
        "    print(f'Target vector size: {batch.sentiment.size()}')\n",
        "    break\n",
        "    \n",
        "print('\\nTest:')\n",
        "for batch in test_loader:\n",
        "    print(f'Text matrix size: {batch.review[0].size()}')\n",
        "    print(f'Target vector size: {batch.sentiment.size()}')\n",
        "    break"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Neuro net model\n",
        "import torch.nn as nn\n",
        "\n",
        "class RNN(nn.Module):\n",
        "    def __init__(self, input_dim, embedding_dim, hidden_dim, output_dim):\n",
        "        \n",
        "        super().__init__()\n",
        "\n",
        "        self.embedding = nn.Embedding(input_dim, embedding_dim)\n",
        "        self.rnn = nn.LSTM(embedding_dim, hidden_dim)\n",
        "        self.fc = nn.Linear(hidden_dim, output_dim)\n",
        "        \n",
        "    def forward(self, text, text_length):\n",
        "\n",
        "        #[sentence len, batch size] => [sentence len, batch size, embedding size]\n",
        "        embedded = self.embedding(text)\n",
        "        \n",
        "        packed = torch.nn.utils.rnn.pack_padded_sequence(embedded, text_length)\n",
        "        \n",
        "        #[sentence len, batch size, embedding size] => \n",
        "        #  output: [sentence len, batch size, hidden size]\n",
        "        #  hidden: [1, batch size, hidden size]\n",
        "        packed_output, (hidden, cell) = self.rnn(packed)\n",
        "        \n",
        "        return self.fc(hidden.squeeze(0)).view(-1) # return 0 or 1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "Ik3NF3faxFmZ"
      },
      "outputs": [],
      "source": [
        "INPUT_DIM = len(TEXT.vocab)\n",
        "\n",
        "torch.manual_seed(RANDOM_SEED)\n",
        "model = RNN(INPUT_DIM, EMBEDDING_DIM, HIDDEN_DIM, OUTPUT_DIM)\n",
        "model = model.to(DEVICE)\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=LEARNING_RATE)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Lv9Ny9di6VcI"
      },
      "source": [
        "## Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "T5t1Afn4xO11"
      },
      "outputs": [],
      "source": [
        "def compute_binary_accuracy(model, data_loader, device):\n",
        "    model.eval()\n",
        "    correct_pred, num_examples = 0, 0\n",
        "    with torch.no_grad():\n",
        "        for batch_idx, batch_data in enumerate(data_loader):\n",
        "            text, text_lengths = batch_data.review\n",
        "            logits = model(text, text_lengths.cpu())\n",
        "            predicted_labels = (torch.sigmoid(logits) > 0.5).long()\n",
        "            num_examples += batch_data.sentiment.size(0)\n",
        "            correct_pred += (predicted_labels.long() == batch_data.sentiment.long()).sum()\n",
        "        return correct_pred.float()/num_examples * 100"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "EABZM8Vo0ilB"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch: 001/025 | Batch 000/012 | Cost: 0.7121\n",
            "training accuracy: 96.59%\n",
            "valid accuracy: 97.49%\n",
            "Time elapsed: 0.01 min\n",
            "Epoch: 002/025 | Batch 000/012 | Cost: 0.1340\n",
            "training accuracy: 96.59%\n",
            "valid accuracy: 97.49%\n",
            "Time elapsed: 0.01 min\n",
            "Epoch: 003/025 | Batch 000/012 | Cost: 0.0605\n",
            "training accuracy: 96.59%\n",
            "valid accuracy: 97.49%\n",
            "Time elapsed: 0.02 min\n",
            "Epoch: 004/025 | Batch 000/012 | Cost: 0.2062\n",
            "training accuracy: 96.59%\n",
            "valid accuracy: 97.49%\n",
            "Time elapsed: 0.03 min\n",
            "Epoch: 005/025 | Batch 000/012 | Cost: 0.1113\n",
            "training accuracy: 96.59%\n",
            "valid accuracy: 97.49%\n",
            "Time elapsed: 0.04 min\n",
            "Epoch: 006/025 | Batch 000/012 | Cost: 0.0866\n",
            "training accuracy: 96.59%\n",
            "valid accuracy: 97.49%\n",
            "Time elapsed: 0.04 min\n",
            "Epoch: 007/025 | Batch 000/012 | Cost: 0.1708\n",
            "training accuracy: 96.59%\n",
            "valid accuracy: 97.49%\n",
            "Time elapsed: 0.05 min\n",
            "Epoch: 008/025 | Batch 000/012 | Cost: 0.1853\n",
            "training accuracy: 96.66%\n",
            "valid accuracy: 97.49%\n",
            "Time elapsed: 0.06 min\n",
            "Epoch: 009/025 | Batch 000/012 | Cost: 0.1069\n",
            "training accuracy: 96.72%\n",
            "valid accuracy: 97.49%\n",
            "Time elapsed: 0.06 min\n",
            "Epoch: 010/025 | Batch 000/012 | Cost: 0.1903\n",
            "training accuracy: 96.72%\n",
            "valid accuracy: 97.49%\n",
            "Time elapsed: 0.07 min\n",
            "Epoch: 011/025 | Batch 000/012 | Cost: 0.2336\n",
            "training accuracy: 96.72%\n",
            "valid accuracy: 97.49%\n",
            "Time elapsed: 0.08 min\n",
            "Epoch: 012/025 | Batch 000/012 | Cost: 0.1709\n",
            "training accuracy: 96.72%\n",
            "valid accuracy: 97.49%\n",
            "Time elapsed: 0.09 min\n",
            "Epoch: 013/025 | Batch 000/012 | Cost: 0.1540\n",
            "training accuracy: 96.72%\n",
            "valid accuracy: 97.49%\n",
            "Time elapsed: 0.09 min\n",
            "Epoch: 014/025 | Batch 000/012 | Cost: 0.1092\n",
            "training accuracy: 96.72%\n",
            "valid accuracy: 97.49%\n",
            "Time elapsed: 0.10 min\n",
            "Epoch: 015/025 | Batch 000/012 | Cost: 0.1036\n",
            "training accuracy: 96.72%\n",
            "valid accuracy: 97.49%\n",
            "Time elapsed: 0.11 min\n",
            "Epoch: 016/025 | Batch 000/012 | Cost: 0.1341\n",
            "training accuracy: 96.72%\n",
            "valid accuracy: 97.49%\n",
            "Time elapsed: 0.11 min\n",
            "Epoch: 017/025 | Batch 000/012 | Cost: 0.0642\n",
            "training accuracy: 96.72%\n",
            "valid accuracy: 97.49%\n",
            "Time elapsed: 0.12 min\n",
            "Epoch: 018/025 | Batch 000/012 | Cost: 0.0821\n",
            "training accuracy: 96.72%\n",
            "valid accuracy: 97.49%\n",
            "Time elapsed: 0.13 min\n",
            "Epoch: 019/025 | Batch 000/012 | Cost: 0.0531\n",
            "training accuracy: 96.72%\n",
            "valid accuracy: 97.49%\n",
            "Time elapsed: 0.14 min\n",
            "Epoch: 020/025 | Batch 000/012 | Cost: 0.1541\n",
            "training accuracy: 96.72%\n",
            "valid accuracy: 97.49%\n",
            "Time elapsed: 0.14 min\n",
            "Epoch: 021/025 | Batch 000/012 | Cost: 0.1595\n",
            "training accuracy: 96.72%\n",
            "valid accuracy: 97.49%\n",
            "Time elapsed: 0.15 min\n",
            "Epoch: 022/025 | Batch 000/012 | Cost: 0.2034\n",
            "training accuracy: 96.72%\n",
            "valid accuracy: 97.49%\n",
            "Time elapsed: 0.16 min\n",
            "Epoch: 023/025 | Batch 000/012 | Cost: 0.0527\n",
            "training accuracy: 96.72%\n",
            "valid accuracy: 97.49%\n",
            "Time elapsed: 0.17 min\n",
            "Epoch: 024/025 | Batch 000/012 | Cost: 0.2060\n",
            "training accuracy: 96.72%\n",
            "valid accuracy: 97.49%\n",
            "Time elapsed: 0.17 min\n",
            "Epoch: 025/025 | Batch 000/012 | Cost: 0.2294\n",
            "training accuracy: 96.72%\n",
            "valid accuracy: 97.49%\n",
            "Time elapsed: 0.18 min\n",
            "Total Training Time: 0.18 min\n",
            "Test accuracy: 92.00%\n"
          ]
        }
      ],
      "source": [
        "start_time = time.time()\n",
        "\n",
        "for epoch in range(NUM_EPOCHS):\n",
        "    model.train()\n",
        "    for batch_idx, batch_data in enumerate(train_loader):\n",
        "        \n",
        "        text, text_lengths = batch_data.review\n",
        "        \n",
        "        ### FORWARD AND BACK PROP\n",
        "        logits = model(text, text_lengths.cpu())\n",
        "        cost = F.binary_cross_entropy_with_logits(logits, batch_data.sentiment)\n",
        "        optimizer.zero_grad()\n",
        "        \n",
        "        cost.backward()\n",
        "        \n",
        "        ### UPDATE MODEL PARAMETERS\n",
        "        optimizer.step()\n",
        "        \n",
        "        ### LOGGING\n",
        "        if not batch_idx % 50:\n",
        "            print (f'Epoch: {epoch+1:03d}/{NUM_EPOCHS:03d} | '\n",
        "                   f'Batch {batch_idx:03d}/{len(train_loader):03d} | '\n",
        "                   f'Cost: {cost:.4f}')\n",
        "\n",
        "    with torch.set_grad_enabled(False):\n",
        "        print(f'training accuracy: '\n",
        "              f'{compute_binary_accuracy(model, train_loader, DEVICE):.2f}%'\n",
        "              f'\\nvalid accuracy: '\n",
        "              f'{compute_binary_accuracy(model, valid_loader, DEVICE):.2f}%')\n",
        "        \n",
        "    print(f'Time elapsed: {(time.time() - start_time)/60:.2f} min')\n",
        "    \n",
        "print(f'Total Training Time: {(time.time() - start_time)/60:.2f} min')\n",
        "print(f'Test accuracy: {compute_binary_accuracy(model, test_loader, DEVICE):.2f}%')"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "name": "Lab4(2).ipynb",
      "provenance": [],
      "toc_visible": true
    },
    "interpreter": {
      "hash": "e95bd23c58372e532c13f19770ac32345ce14fa00d207afebbf1161aa1e79f4b"
    },
    "kernelspec": {
      "display_name": "Python 3.8.10 64-bit ('base': conda)",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.10"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}